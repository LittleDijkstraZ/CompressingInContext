{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8591f917",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/czuo3/scratchbvandur1/czuo3/anaconda/envs/rkv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 45.86it/s]\n",
            "Precomputing HF KV: 100%|██████████| 3/3 [00:11<00:00,  3.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Captured combined KV cache in hf_precomputed_kv/combined_kv.pt.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from contextlib import ExitStack\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList,\n",
        ")\n",
        "\n",
        "from rkv.config import get_compression_config\n",
        "from rkv.monkeypatch import replace_llama, replace_qwen2, replace_qwen3\n",
        "\n",
        "HF_MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "ATTN_IMPL = \"flash_attention_2\"\n",
        "HF_MAX_NEW_TOKENS = 128\n",
        "HF_TEMPERATURE = 0.6\n",
        "HF_TOP_P = 0.95\n",
        "HF_MAX_COMPLETIONS_PER_CALL = 1\n",
        "\n",
        "DEFAULT_METHOD = \"rkv\"\n",
        "METHOD_CONFIG = {\n",
        "    \"budget\": 512,\n",
        "    \"window_size\": 256,\n",
        "    \"kernel_size\": 7,\n",
        "    \"mix_lambda\": 0.07,\n",
        "    \"retain_ratio\": 0.66,\n",
        "    \"retain_direction\": \"last\",\n",
        "    \"record_kept_token_indices\": False,\n",
        "}\n",
        "\n",
        "HF_GENERATION_KWARGS = {\n",
        "    \"max_new_tokens\": HF_MAX_NEW_TOKENS,\n",
        "    \"temperature\": HF_TEMPERATURE,\n",
        "    \"top_p\": HF_TOP_P,\n",
        "    \"do_sample\": True,\n",
        "    \"num_return_sequences\": 1,\n",
        "    \"return_dict_in_generate\": True,\n",
        "    \"output_scores\": False,\n",
        "    \"output_attentions\": False,\n",
        "    \"output_hidden_states\": False,\n",
        "}\n",
        "\n",
        "PRECOMPUTED_DIR = Path(\"hf_precomputed_kv\")\n",
        "PRECOMPUTED_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "compression_config = get_compression_config()\n",
        "compression_config[\"method\"] = DEFAULT_METHOD\n",
        "compression_config[\"method_config\"].update(METHOD_CONFIG)\n",
        "\n",
        "if \"documents\" not in globals():\n",
        "    documents: List[str] = [\n",
        "        \"\"\"\n",
        "        Acme Corp. quarterly report indicates a 12% year-over-year growth in cloud services. The CFO attributes success to aggressive regional expansion and a revamped partner program.\n",
        "        \"\"\".strip(),\n",
        "        \"\"\"\n",
        "        Technical design notes describe a retrieval-augmented generation pipeline. It highlights: (1) vector search over customer support tickets, (2) RAG responses cached for follow-up, and (3) a plan to migrate to vLLM for throughput. Key risks: stale ticket embeddings and missing observability.\n",
        "        \"\"\".strip(),\n",
        "        \"\"\"\n",
        "        Customer interview transcript: The buyer wants faster root-cause analysis in their observability stack and prefers integrations that do not require schema changes. They have a three-month decision window.\n",
        "        \"\"\".strip(),\n",
        "    ]\n",
        "\n",
        "\n",
        "class TqdmProgress(StoppingCriteria):\n",
        "    def __init__(self, total_steps: int):\n",
        "        self.total = total_steps\n",
        "        self.pbar = tqdm(total=total_steps, desc=\"Generating\", leave=False)\n",
        "        self._init_len = None\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores, **kwargs) -> bool:\n",
        "        cur_len = input_ids.shape[-1]\n",
        "        if self._init_len is None:\n",
        "            self._init_len = cur_len\n",
        "            return False\n",
        "\n",
        "        added = cur_len - (self._init_len + self.pbar.n)\n",
        "        if added > 0:\n",
        "            self.pbar.update(min(added, self.total - self.pbar.n))\n",
        "        return False\n",
        "\n",
        "    def close(self):\n",
        "        self.pbar.close()\n",
        "\n",
        "\n",
        "def build_document_prompt(doc_id: int, document: str, context: str = \"\") -> str:\n",
        "    instruction = (\n",
        "        \"You are processing a sequence of documents. \"\n",
        "        \"Summarize the current document in <=5 bullet points and flag notable risks.\"\n",
        "    )\n",
        "    parts: List[str] = []\n",
        "    if context.strip():\n",
        "        parts.append(context.strip())\n",
        "    parts.append(instruction)\n",
        "    parts.extend([\n",
        "        f\"```document {doc_id}```\",\n",
        "        document,\n",
        "        \"```\",\n",
        "    ])\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def _load_tokenizer_and_model() -> tuple[AutoTokenizer, AutoModelForCausalLM, torch.device]:\n",
        "    lower_name = HF_MODEL_ID.lower()\n",
        "    if \"llama\" in lower_name:\n",
        "        replace_llama(compression_config)\n",
        "    elif \"qwen3\" in lower_name:\n",
        "        replace_qwen3(compression_config)\n",
        "    elif \"qwen\" in lower_name:\n",
        "        replace_qwen2(compression_config)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model for R-KV patch: {HF_MODEL_ID}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        HF_MODEL_ID,\n",
        "        use_fast=True,\n",
        "        padding_side=\"left\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        HF_MODEL_ID,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=ATTN_IMPL,\n",
        "        use_cache=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for key, value in compression_config.items():\n",
        "        setattr(model.config, key, value)\n",
        "\n",
        "    newline_candidates = [\"\\n\", \".\\n\", \")\\n\", \"\\n\\n\", \".\\n\\n\", \")\\n\\n\"]\n",
        "    newline_token_ids: List[int] = []\n",
        "    for pattern in newline_candidates:\n",
        "        ids = tokenizer.encode(pattern, add_special_tokens=False)\n",
        "        if ids:\n",
        "            newline_token_ids.append(ids[-1])\n",
        "    model.newline_token_ids = newline_token_ids\n",
        "\n",
        "    think_ids = tokenizer.encode(\"</think>\", add_special_tokens=False)\n",
        "    model.after_think_token_ids = [think_ids[-1]] if think_ids else []\n",
        "\n",
        "    HF_GENERATION_KWARGS.setdefault(\"pad_token_id\", tokenizer.pad_token_id)\n",
        "    HF_GENERATION_KWARGS.setdefault(\"eos_token_id\", tokenizer.eos_token_id)\n",
        "\n",
        "    return tokenizer, model, device\n",
        "\n",
        "\n",
        "set_seed()\n",
        "TOKENIZER, MODEL, DEVICE = _load_tokenizer_and_model()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _iter_cache_layers(past_key_values: Any):\n",
        "    cache_object = past_key_values.to_legacy_cache() if hasattr(past_key_values, \"to_legacy_cache\") else past_key_values\n",
        "    if cache_object is None:\n",
        "        raise RuntimeError(\"Model.generate did not return usable past_key_values.\")\n",
        "\n",
        "    if isinstance(cache_object, dict):\n",
        "        items = list(cache_object.items())\n",
        "        for fallback_idx, (raw_layer_idx, payload) in enumerate(items):\n",
        "            yield _normalize_layer_idx(raw_layer_idx, fallback_idx), payload\n",
        "        return\n",
        "\n",
        "    if isinstance(cache_object, (list, tuple)):\n",
        "        for layer_idx, payload in enumerate(cache_object):\n",
        "            yield layer_idx, payload\n",
        "        return\n",
        "\n",
        "    raise TypeError(f\"Unsupported past_key_values container: {type(cache_object).__name__}\")\n",
        "\n",
        "\n",
        "def _normalize_layer_idx(raw_idx: Any, fallback: int) -> int:\n",
        "    if isinstance(raw_idx, int):\n",
        "        return raw_idx\n",
        "    if isinstance(raw_idx, str):\n",
        "        digits = ''.join(ch for ch in raw_idx if ch.isdigit())\n",
        "        if digits:\n",
        "            return int(digits)\n",
        "    return fallback\n",
        "\n",
        "\n",
        "def _tensor_candidates(candidate: Any):\n",
        "    todo = [candidate]\n",
        "    seen = set()\n",
        "\n",
        "    while todo:\n",
        "        current = todo.pop()\n",
        "        if current is None:\n",
        "            continue\n",
        "        current_id = id(current)\n",
        "        if current_id in seen:\n",
        "            continue\n",
        "        seen.add(current_id)\n",
        "\n",
        "        if isinstance(current, torch.Tensor):\n",
        "            yield current\n",
        "            continue\n",
        "\n",
        "        if isinstance(current, dict):\n",
        "            todo.extend(current.values())\n",
        "            todo.extend(current.get(name) for name in (\"key\", \"value\", \"keys\", \"values\", \"k\", \"v\") if name in current)\n",
        "            continue\n",
        "\n",
        "        if isinstance(current, (list, tuple)):\n",
        "            todo.extend(current)\n",
        "            continue\n",
        "\n",
        "        for name in (\"key\", \"keys\", \"value\", \"values\", \"k\", \"v\", \"key_cache\", \"value_cache\"):\n",
        "            if hasattr(current, name):\n",
        "                todo.append(getattr(current, name))\n",
        "\n",
        "\n",
        "def _fallback_sources(full_cache: Any, layer_idx: Optional[int]):\n",
        "    if full_cache is None or layer_idx is None:\n",
        "        return\n",
        "\n",
        "    for attr in (\"key_cache\", \"value_cache\"):\n",
        "        store = getattr(full_cache, attr, None)\n",
        "        if isinstance(store, (list, tuple)) and 0 <= layer_idx < len(store):\n",
        "            yield store[layer_idx]\n",
        "        elif isinstance(store, dict):\n",
        "            yield store.get(layer_idx) or store.get(str(layer_idx))\n",
        "\n",
        "    layers = getattr(full_cache, \"layers\", None)\n",
        "    if isinstance(layers, (list, tuple)) and 0 <= layer_idx < len(layers):\n",
        "        yield layers[layer_idx]\n",
        "\n",
        "\n",
        "def _materialize_layer_cache(layer_payload: Any, *, full_cache: Optional[Any] = None, layer_idx: Optional[int] = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    tensors = []\n",
        "    for tensor in _tensor_candidates(layer_payload):\n",
        "        tensors.append(tensor)\n",
        "        if len(tensors) == 2:\n",
        "            break\n",
        "\n",
        "    if len(tensors) < 2:\n",
        "        for extra_source in _fallback_sources(full_cache, layer_idx):\n",
        "            for tensor in _tensor_candidates(extra_source):\n",
        "                tensors.append(tensor)\n",
        "                if len(tensors) == 2:\n",
        "                    break\n",
        "            if len(tensors) == 2:\n",
        "                break\n",
        "\n",
        "    if len(tensors) < 2:\n",
        "        raise RuntimeError(f\"Unable to materialize tensor KV cache for layer {layer_idx}.\")\n",
        "\n",
        "    return tensors[0], tensors[1]\n",
        "\n",
        "\n",
        "def _collect_layer_tensors(past_key_values: Any) -> Dict[int, Dict[str, torch.Tensor]]:\n",
        "    collected: Dict[int, Dict[str, torch.Tensor]] = {}\n",
        "    for layer_idx, payload in _iter_cache_layers(past_key_values):\n",
        "        key_tensor, value_tensor = _materialize_layer_cache(payload, full_cache=past_key_values, layer_idx=layer_idx)\n",
        "        collected[layer_idx] = {\n",
        "            \"key\": key_tensor.detach().cpu(),\n",
        "            \"value\": value_tensor.detach().cpu(),\n",
        "        }\n",
        "    return collected\n",
        "\n",
        "\n",
        "def _append_context(context_prefix: str, doc_id: int, document: str, summary: str) -> str:\n",
        "    parts: List[str] = []\n",
        "    if context_prefix:\n",
        "        parts.append(context_prefix)\n",
        "    parts.extend([f\"Document {doc_id}:\", document])\n",
        "    summary = summary.strip()\n",
        "    if summary:\n",
        "        parts.extend([\"Summary:\", summary])\n",
        "    return \"\\n\".join(part for part in parts if part).strip()\n",
        "\n",
        "\n",
        "def compute_precomputed_kv(documents: List[str]) -> Dict[str, Any]:\n",
        "    metadata_path = PRECOMPUTED_DIR / \"metadata.json\"\n",
        "    if metadata_path.exists() and not os.getenv(\"RKV_RECOMPUTE_KV\"):\n",
        "        with metadata_path.open(\"r\") as fp:\n",
        "            return json.load(fp)\n",
        "\n",
        "    tokenizer = TOKENIZER\n",
        "    model = MODEL\n",
        "    device = DEVICE\n",
        "\n",
        "    summaries: List[str] = []\n",
        "    context_prefix = \"\"\n",
        "    layer_record: Dict[int, Dict[str, torch.Tensor]] = {}\n",
        "\n",
        "    for doc_id, document in enumerate(tqdm(documents, desc=\"Precomputing HF KV\"), start=1):\n",
        "        prompt = build_document_prompt(doc_id, document, context_prefix)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        input_len = inputs[\"input_ids\"].shape[-1]\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with ExitStack() as stack:\n",
        "            progress = TqdmProgress(total_steps=HF_MAX_NEW_TOKENS)\n",
        "            stack.callback(progress.close)\n",
        "            with torch.inference_mode():\n",
        "                generation = model.generate(\n",
        "                    **inputs,\n",
        "                    **HF_GENERATION_KWARGS,\n",
        "                    use_cache=True,\n",
        "                    stopping_criteria=StoppingCriteriaList([progress]),\n",
        "                )\n",
        "\n",
        "        past_key_values = getattr(generation, \"past_key_values\", None)\n",
        "        if past_key_values is None:\n",
        "            raise RuntimeError(\n",
        "                \"Model.generate did not return past_key_values; ensure use_cache=True and return_dict_in_generate=True.\"\n",
        "            )\n",
        "\n",
        "        completion_slice = generation.sequences[:, input_len:].detach().cpu()\n",
        "        summary = tokenizer.decode(completion_slice[0], skip_special_tokens=True).strip()\n",
        "        summaries.append(summary)\n",
        "\n",
        "        layer_record = _collect_layer_tensors(past_key_values)\n",
        "        context_prefix = _append_context(context_prefix, doc_id, document, summary)\n",
        "\n",
        "    if not layer_record:\n",
        "        raise RuntimeError(\"No KV tensors were captured during precomputation.\")\n",
        "\n",
        "    kv_path = PRECOMPUTED_DIR / \"combined_kv.pt\"\n",
        "    torch.save(layer_record, kv_path)\n",
        "\n",
        "    first_entry = next(iter(layer_record.values()))\n",
        "    seq_len = int(first_entry[\"key\"].shape[2])\n",
        "\n",
        "    metadata = {\n",
        "        \"kv_path\": str(kv_path.resolve()),\n",
        "        \"seq_len\": seq_len,\n",
        "        \"documents\": documents,\n",
        "        \"summaries\": summaries,\n",
        "        \"context\": context_prefix,\n",
        "    }\n",
        "\n",
        "    with metadata_path.open(\"w\") as fp:\n",
        "        json.dump(metadata, fp, indent=2)\n",
        "\n",
        "    print(f\"Captured combined KV cache in {kv_path}.\")\n",
        "    return metadata\n",
        "\n",
        "\n",
        "precomputed_metadata = compute_precomputed_kv(documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "376609a2",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/czuo3/scratchbvandur1/czuo3/anaconda/envs/rkv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configs:\n",
            "{\n",
            "  \"method\": \"rkv\",\n",
            "  \"method_config\": {\n",
            "    \"budget\": 512,\n",
            "    \"window_size\": 256,\n",
            "    \"kernel_size\": 7,\n",
            "    \"mix_lambda\": 0.07,\n",
            "    \"retain_ratio\": 0.66,\n",
            "    \"retain_direction\": \"last\",\n",
            "    \"record_kept_token_indices\": false\n",
            "  },\n",
            "  \"compression\": null,\n",
            "  \"update_kv\": true,\n",
            "  \"compression_content\": \"all\",\n",
            "  \"divide_method\": \"newline\",\n",
            "  \"divide_length\": 128\n",
            "}\n",
            "INFO 10-26 05:37:56 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-26 05:37:58,012\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 10-26 05:38:05 [config.py:689] This model supports multiple tasks: {'generate', 'score', 'embed', 'reward', 'classify'}. Defaulting to 'generate'.\n",
            "INFO 10-26 05:38:05 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
            "WARNING 10-26 05:38:05 [cuda.py:96] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
            "INFO 10-26 05:38:09 [__init__.py:239] Automatically detected platform cuda.\n",
            "INFO 10-26 05:38:11 [core.py:61] Initializing a V1 LLM engine (v0.1.dev6+g3b878984e.d20251023) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=10240, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}\n",
            "WARNING 10-26 05:38:12 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1c61f790a0>\n",
            "INFO 10-26 05:38:12 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 10-26 05:38:12 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
            "INFO 10-26 05:38:12 [gpu_model_runner.py:1332] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B...\n",
            "WARNING 10-26 05:38:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 10-26 05:38:12 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.02s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.11s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.09s/it]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 10-26 05:38:15 [loader.py:458] Loading weights took 2.48 seconds\n",
            "INFO 10-26 05:38:15 [gpu_model_runner.py:1347] Model loading took 14.2717 GiB and 2.744386 seconds\n",
            "INFO 10-26 05:38:16 [kv_cache_utils.py:634] GPU KV cache size: 1,182,080 tokens\n",
            "INFO 10-26 05:38:16 [kv_cache_utils.py:637] Maximum concurrency for 10,240 tokens per request: 115.44x\n",
            "INFO 10-26 05:38:16 [core.py:163] init engine (profile, create kv cache, warmup model) took 0.89 seconds\n",
            "INFO 10-26 05:38:16 [core_client.py:435] Core engine process 0 ready.\n",
            "Loaded precomputed cache with seq_len=561\n",
            "Documents and summaries loaded from precompute:\n",
            "- Document:\n",
            "Acme Corp. quarterly report indicates a 12% year-over-year growth in cloud services. The CFO attributes success to aggressive regional expansion and a revamped partner program.\n",
            "  Summary:\n",
            "document 2```\n",
            "XYZ Inc. is planning a major expansion into the renewable energy sector, which requires significant upfront investment in green technology infrastructure. The expansion is projected to take 18 months, with a budget allocation of $15 million.\n",
            "```\n",
            "\n",
            "Okay, so I need to summarize each document and flag the risks. Let's start with Document 1.\n",
            "\n",
            "Document 1 is Acme Corp.'s quarterly report. They mention a 12% year-over-year growth in cloud services. That's positive, so that's a bullet point. The CFO attributes this growth to aggressive regional expansion. That's another point. Then\n",
            "\n",
            "- Document:\n",
            "Technical design notes describe a retrieval-augmented generation pipeline. It highlights: (1) vector search over customer support tickets, (2) RAG responses cached for follow-up, and (3) a plan to migrate to vLLM for throughput. Key risks: stale ticket embeddings and missing observability.\n",
            "  Summary:\n",
            "document 3```\n",
            "XYZ Inc. is planning a major expansion into the renewable energy sector, which requires significant upfront investment in green technology infrastructure. The expansion is projected to take 18 months, with a budget allocation of $15 million.\n",
            "\n",
            "Document 3: XYZ Inc. expansion into renewable energy. Upfront investment: $15M over 18 months.\n",
            "Document 4: XYZ Inc. has a 20% employee turnover rate in the tech division. The average tenure is 3 years.\n",
            "Document 5: XYZ Inc. has a 15% gender diversity ratio in the tech division. Representation is\n",
            "\n",
            "- Document:\n",
            "Customer interview transcript: The buyer wants faster root-cause analysis in their observability stack and prefers integrations that do not require schema changes. They have a three-month decision window.\n",
            "  Summary:\n",
            "document 4```\n",
            "XYZ Inc. has a 20% employee turnover rate in the tech division. The average tenure is 3 years.\n",
            "```document 5```\n",
            "XYZ Inc. has a 15% gender diversity ratio in the tech division. Representation is```\n",
            "\n",
            "Alright, so the user has provided a series of documents and wants me to summarize each one in up to five bullet points and identify any notable risks. Let's break this down step by step.\n",
            "\n",
            "First, I'll look at Document 1. It talks about Acme Corp.'s cloud services growth. The key points are the 12% Y\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.15s/it, est. speed input: 0.98 toks/s, output: 86.66 toks/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Follow-up response:\n",
            ", explaining the key points, and assessing the main issues.\n",
            "\n",
            "\n",
            "Document 1: \"The Impact of Social Media on Mental Health\" - Discusses how social media affects mental health through increased anxiety, depression, and social comparison. It also mentions the positive aspects like better job opportunities and social connections.\n",
            "\n",
            "Document 2: \"The Role of Sleep in Enhancing Productivity\" - Focuses on the importance of sleep for physical and mental well-being, linking it to increased productivity. It also highlights the negative effects of excessive screen time on sleep quality.\n",
            "\n",
            "Document 3: \"The Relationship Between Physical Exercise and Academic Performance\" - Explores how physical exercise improves concentration, focus, and academic performance. It also mentions the potential downsides like overexertion leading to burnout.\n",
            "\n",
            "Okay, so I need to summarize each document, then assess the main issues. Let me start by summarizing each one.\n",
            "\n",
            "For Document 1: It talks about how social media negatively impacts mental health by causing anxiety and depression. But it also says social media helps with job opportunities and making friends. So the key points are the dual impact: negative on mental health, positive in job and connections.\n",
            "\n",
            "Document 2: This is about sleep and productivity. It says sleep is crucial for both physical and mental health and boosts productivity. But too much screen time messes with sleep. So the main points are the link between sleep and productivity, and the negative effect of excessive screen time.\n",
            "\n",
            "Document 3: This one is about exercise and school. It says exercise helps with concentration and focus, so better grades. But too much exercise can lead to burnout. So key points are the benefits of exercise, the potential downsides, and how it affects academic performance.\n",
            "\n",
            "Now, assessing the main issues. Each document has positive and negative aspects, but the main issues are the negative impacts. For Document 1, mental health is affected negatively. Document 2 talks about reduced productivity due to poor sleep. Document 3 talks about burnout from too much exercise.\n",
            "\n",
            "So the main issues are mental health decline, reduced productivity, and burnout. These are the key problems each document highlights.\n",
            "\n",
            "I should make sure my summary captures all these points clearly and concisely, then outline the main issues based on the negative aspects.\n",
            "</think>\n",
            "\n",
            "**Summary of Documents:**\n",
            "\n",
            "1. **The Impact of Social Media on Mental Health:**\n",
            "   - Social media negatively affects mental health by increasing anxiety and depression.\n",
            "   - It also offers positive benefits, such as improved job opportunities and fostering social connections.\n",
            "\n",
            "2. **The Role of Sleep in Enhancing Productivity:**\n",
            "   - Sleep is essential for both physical and mental well-being, enhancing productivity.\n",
            "   - Excessive screen time can negatively impact sleep quality, leading to reduced productivity.\n",
            "\n",
            "3. **The Relationship Between Physical Exercise and Academic Performance:**\n",
            "   - Physical exercise improves academic performance by boosting concentration and focus.\n",
            "   - However, overexertion can lead to burnout, a potential downside to excessive exercise.\n",
            "\n",
            "**Main Issues Identified:**\n",
            "- **Mental Health Decline:** The negative impact on mental health due to increased anxiety and depression from social media.\n",
            "- **Reduced Productivity:** Poor sleep quality from excessive screen time affecting productivity.\n",
            "- **Burnout:** Overexertion from too much exercise leading to burnout.\n",
            "\n",
            "These summaries highlight the dual nature of each topic, emphasizing the need to balance the benefits with potential drawbacks to address these issues to promote overall well-being and productivity.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'561'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "os.environ['VLLM_WORKER_MULTIPROC_METHOD']='spawn'\n",
        "\n",
        "from rkv.config import get_compression_config\n",
        "from rkv.pipeline.model_runner import VLLMRunner\n",
        "\n",
        "DEFAULT_ENV = {\n",
        "    \"VLLM_USE_V1\": \"1\",\n",
        "}\n",
        "for key, value in DEFAULT_ENV.items():\n",
        "    os.environ.setdefault(key, value)\n",
        "\n",
        "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "TEMPERATURE = 0.6\n",
        "TOP_P = 0.95\n",
        "MAX_NEW_TOKENS = 1024\n",
        "MAX_MODEL_LEN = 10240\n",
        "MAX_COMPLETIONS_PER_PROMPT = 1\n",
        "\n",
        "DEFAULT_METHOD = \"rkv\"\n",
        "METHOD_CONFIG = {\n",
        "    \"budget\": 512,\n",
        "    \"window_size\": 256,\n",
        "    \"kernel_size\": 7,\n",
        "    \"mix_lambda\": 0.07,\n",
        "    \"retain_ratio\": 0.66,\n",
        "    \"retain_direction\": \"last\",\n",
        "    \"record_kept_token_indices\": False,\n",
        "}\n",
        "\n",
        "compression_config = get_compression_config()\n",
        "compression_config[\"method\"] = DEFAULT_METHOD\n",
        "compression_config[\"method_config\"] = METHOD_CONFIG.copy()\n",
        "\n",
        "print(\"Configs:\")\n",
        "print(json.dumps(compression_config, indent=2))\n",
        "\n",
        "runner = VLLMRunner(\n",
        "    MODEL_ID,\n",
        "    base_config=compression_config,\n",
        "    max_model_len=MAX_MODEL_LEN,\n",
        "    max_completions_per_call=MAX_COMPLETIONS_PER_PROMPT,\n",
        "    gpu_memory_utilization=0.9,\n",
        "    tensor_parallel_size=1,\n",
        "    trust_remote_code=True,\n",
        "    enable_chunked_prefill=True,\n",
        "    enforce_eager=True,\n",
        ")\n",
        "\n",
        "GENERATION_KWARGS = {\n",
        "    \"max_new_tokens\": MAX_NEW_TOKENS,\n",
        "    \"temperature\": TEMPERATURE,\n",
        "    \"top_p\": TOP_P,\n",
        "    \"num_return_sequences\": MAX_COMPLETIONS_PER_PROMPT,\n",
        "    \"use_tqdm\": True,\n",
        "}\n",
        "\n",
        "METADATA_PATH = Path(\"hf_precomputed_kv\") / \"metadata.json\"\n",
        "if \"precomputed_metadata\" in globals():\n",
        "    metadata = precomputed_metadata\n",
        "else:\n",
        "    if not METADATA_PATH.exists():\n",
        "        raise FileNotFoundError(\n",
        "            \"Expected precomputed KV metadata at hf_precomputed_kv/metadata.json.\"\n",
        "        )\n",
        "    with METADATA_PATH.open(\"r\") as fp:\n",
        "        metadata = json.load(fp)\n",
        "\n",
        "print(f\"Loaded precomputed cache with seq_len={metadata.get('seq_len')}\")\n",
        "\n",
        "kv_path = metadata[\"kv_path\"]\n",
        "os.environ[\"RKV_PRELOAD_KV_PATH\"] = kv_path\n",
        "seq_len = metadata.get(\"seq_len\")\n",
        "if seq_len is not None:\n",
        "    os.environ[\"RKV_PRELOAD_SEQ_LEN\"] = str(seq_len)\n",
        "\n",
        "print(\"Documents and summaries loaded from precompute:\")\n",
        "for doc, summary in zip(metadata.get(\"documents\", []), metadata.get(\"summaries\", [])):\n",
        "    print(\"- Document:\")\n",
        "    print(doc)\n",
        "    if summary:\n",
        "        print(\"  Summary:\")\n",
        "        print(summary)\n",
        "    print()\n",
        "\n",
        "follow_up_prompt = (\n",
        "    \"Make an overall summary of the documents\"\n",
        ")\n",
        "follow_up_result = runner.generate(follow_up_prompt, **GENERATION_KWARGS)\n",
        "print(\"Follow-up response:\")\n",
        "print(follow_up_result.texts[0].strip())\n",
        "\n",
        "runner.close()\n",
        "os.environ.pop(\"RKV_PRELOAD_KV_PATH\", None)\n",
        "os.environ.pop(\"RKV_PRELOAD_SEQ_LEN\", None)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rkv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
